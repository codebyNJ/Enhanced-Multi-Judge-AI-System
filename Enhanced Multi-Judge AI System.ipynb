{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14c8b25a"
      },
      "source": [
        "# Enhanced Multi-Judge AI System\n",
        "\n",
        "This notebook contains an enhanced multi-judge AI system designed to leverage multiple language models for comprehensive query processing, evaluation, and synthesis.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "- **Parallel Response Generation:** Generates responses from multiple models simultaneously.\n",
        "- **Advanced Evaluation:** Employs detailed metrics (factual accuracy, coherence, relevance, etc.) and statistical analysis to evaluate model performance.\n",
        "- **Targeted Regeneration:** Improves low-scoring responses based on evaluation feedback.\n",
        "- **Collaborative Discussion Simulation:** Assigns roles (Knowledge Integrator, Perspective Analyst, Clarifier, Synthesizer) to models to simulate a discussion and generate a synthesized consensus answer.\n",
        "- **Benchmark System:** Includes a comprehensive benchmark system to statistically evaluate model performance across different question categories and difficulties.\n",
        "- **Optimized Debate System:** A faster, albeit simpler, system for rapid response generation and evaluation.\n",
        "\n",
        "The system aims to provide more robust and reliable answers by aggregating insights and mitigating individual model weaknesses through a structured process."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main version"
      ],
      "metadata": {
        "id": "hJQxnnfkm8mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language_tool_python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4rEU43_RPCi",
        "outputId": "7d6c0251-a999-4726-ace2-fd8881ebcfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.9.4-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (0.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (2025.7.14)\n",
            "Downloading language_tool_python-2.9.4-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: language_tool_python\n",
            "Successfully installed language_tool_python-2.9.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Deque, Tuple\n",
        "import aiohttp\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "import time\n",
        "from collections import deque, defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import statistics\n",
        "import re\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from difflib import SequenceMatcher\n",
        "import random\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class KnowledgeGapAnalysis:\n",
        "    \"\"\"Analysis of knowledge gaps in model responses\"\"\"\n",
        "    missing_keywords: List[str] = field(default_factory=list)\n",
        "    factual_discrepancies: List[str] = field(default_factory=list)\n",
        "    logical_inconsistencies: List[str] = field(default_factory=list)\n",
        "    semantic_gaps: float = 0.0\n",
        "    structural_differences: float = 0.0\n",
        "\n",
        "@dataclass\n",
        "class DiscussionMetrics:\n",
        "    \"\"\"Comprehensive metrics for model evaluation\"\"\"\n",
        "    factual_accuracy: float = 0.0\n",
        "    logical_consistency: float = 0.0\n",
        "    reference_alignment: float = 0.0\n",
        "    coherence_score: float = 0.0\n",
        "    completeness_score: float = 0.0\n",
        "    relevance_score: float = 0.0\n",
        "    readability_score: float = 0.0\n",
        "    grammar_score: float = 0.0\n",
        "    vocabulary_diversity: float = 0.0\n",
        "    bleu_score: float = 0.0\n",
        "    semantic_similarity: float = 0.0\n",
        "    response_time: float = 0.0\n",
        "    token_efficiency: float = 0.0\n",
        "    confidence_interval: Tuple[float, float] = (0.0, 0.0)\n",
        "    standard_error: float = 0.0\n",
        "    knowledge_gaps: KnowledgeGapAnalysis = field(default_factory=KnowledgeGapAnalysis)\n",
        "    overall_score: float = 0.0\n",
        "    category_scores: Dict[str, float] = field(default_factory=dict)\n",
        "    collaboration_score: float = 0.0\n",
        "\n",
        "@dataclass\n",
        "class DiscussionQuestion:\n",
        "    \"\"\"Standardized discussion question structure\"\"\"\n",
        "    id: str\n",
        "    question: str\n",
        "    category: str\n",
        "    difficulty: str\n",
        "    reference_answer: str\n",
        "    evaluation_criteria: List[str]\n",
        "    expected_keywords: List[str] = field(default_factory=list)\n",
        "    max_tokens: int = 1500\n",
        "    perspectives: List[str] = field(default_factory=list)\n",
        "\n",
        "class DiscussionDataset:\n",
        "    \"\"\"Standardized discussion questions across different domains\"\"\"\n",
        "    @staticmethod\n",
        "    def get_comprehensive_dataset() -> List[DiscussionQuestion]:\n",
        "        return [\n",
        "            DiscussionQuestion(\n",
        "                id=\"ethics_001\",\n",
        "                question=\"What are the main ethical concerns regarding AI decision-making in healthcare?\",\n",
        "                category=\"ethics\",\n",
        "                difficulty=\"hard\",\n",
        "                reference_answer=\"Key concerns include patient privacy, algorithmic bias, accountability for decisions, transparency in AI reasoning, consent for AI involvement, and ensuring human oversight in critical decisions.\",\n",
        "                evaluation_criteria=[\"completeness\", \"relevance\", \"coherence\"],\n",
        "                expected_keywords=[\"privacy\", \"bias\", \"accountability\", \"transparency\", \"consent\", \"oversight\"],\n",
        "                perspectives=[\"patient perspective\", \"medical professional perspective\", \"AI developer perspective\", \"regulatory perspective\"]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "class AdvancedEvaluationSystem:\n",
        "    def __init__(self):\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        self.smoothing = SmoothingFunction().method1\n",
        "        self.knowledge_cache = defaultdict(dict)\n",
        "\n",
        "    def calculate_comprehensive_metrics(self, response: str, reference: str,\n",
        "                                     question: DiscussionQuestion,\n",
        "                                     response_time: float) -> DiscussionMetrics:\n",
        "        metrics = DiscussionMetrics()\n",
        "        metrics.response_time = response_time\n",
        "        metrics.token_efficiency = len(response.split()) / max(response_time, 0.1)\n",
        "\n",
        "        # Only calculate metrics if we have a reference answer\n",
        "        if reference:\n",
        "            metrics.factual_accuracy = self._calculate_factual_accuracy(response, reference, question)\n",
        "            metrics.logical_consistency = self._calculate_logical_consistency(response)\n",
        "            metrics.reference_alignment = self._calculate_reference_alignment(response, reference)\n",
        "            metrics.completeness_score = self._calculate_completeness(response, question)\n",
        "            metrics.semantic_similarity = self._calculate_semantic_similarity(response, reference)\n",
        "            metrics.bleu_score = self._calculate_bleu_score(response, reference)\n",
        "            metrics.knowledge_gaps = self._analyze_knowledge_gaps(response, reference, question)\n",
        "            metrics.collaboration_score = self._calculate_collaboration_score(response, question)\n",
        "        else:\n",
        "            # Set high default values when no reference is available\n",
        "            metrics.factual_accuracy = 1.0\n",
        "            metrics.logical_consistency = 1.0\n",
        "            metrics.reference_alignment = 1.0\n",
        "            metrics.completeness_score = 1.0\n",
        "            metrics.semantic_similarity = 1.0\n",
        "            metrics.bleu_score = 1.0\n",
        "            metrics.collaboration_score = 1.0\n",
        "            metrics.knowledge_gaps = KnowledgeGapAnalysis()\n",
        "\n",
        "        # Always calculate these metrics\n",
        "        metrics.coherence_score = self._calculate_coherence(response)\n",
        "        metrics.relevance_score = self._calculate_relevance(response, question.question)\n",
        "        metrics.readability_score = self._calculate_readability(response)\n",
        "        metrics.grammar_score = self._calculate_grammar_score(response)\n",
        "        metrics.vocabulary_diversity = self._calculate_vocabulary_diversity(response)\n",
        "\n",
        "        metrics.overall_score = self._calculate_overall_score(metrics)\n",
        "        return metrics\n",
        "\n",
        "    def _calculate_collaboration_score(self, response: str, question: DiscussionQuestion) -> float:\n",
        "        if not question.perspectives:\n",
        "            return 1.0\n",
        "\n",
        "        perspective_matches = sum(\n",
        "            1 for perspective in question.perspectives\n",
        "            if perspective.lower() in response.lower()\n",
        "        )\n",
        "        return perspective_matches / len(question.perspectives)\n",
        "\n",
        "    def _analyze_knowledge_gaps(self, response: str, reference: str,\n",
        "                               question: DiscussionQuestion) -> KnowledgeGapAnalysis:\n",
        "        analysis = KnowledgeGapAnalysis()\n",
        "        if not reference:\n",
        "            return analysis\n",
        "\n",
        "        response_lower = response.lower()\n",
        "        analysis.missing_keywords = [\n",
        "            kw for kw in question.expected_keywords\n",
        "            if kw.lower() not in response_lower\n",
        "        ]\n",
        "\n",
        "        analysis.structural_differences = 1 - SequenceMatcher(\n",
        "            None, response, reference\n",
        "        ).ratio()\n",
        "\n",
        "        ref_sentences = [s.strip() for s in reference.split('.') if s.strip()]\n",
        "        resp_sentences = [s.strip() for s in response.split('.') if s.strip()]\n",
        "\n",
        "        for i, (ref, resp) in enumerate(zip(ref_sentences, resp_sentences)):\n",
        "            if i >= len(resp_sentences):\n",
        "                break\n",
        "            if (\"not \" + ref.lower() in resp.lower() or\n",
        "                ref.lower() in \"not \" + resp.lower()):\n",
        "                analysis.factual_discrepancies.append(\n",
        "                    f\"Contradiction in sentence {i+1}: '{ref}' vs '{resp}'\"\n",
        "                )\n",
        "\n",
        "        try:\n",
        "            docs = [response, reference]\n",
        "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(docs)\n",
        "            analysis.semantic_gaps = 1 - cosine_similarity(\n",
        "                tfidf_matrix[0:1], tfidf_matrix[1:2]\n",
        "            )[0][0]\n",
        "        except:\n",
        "            analysis.semantic_gaps = 1.0\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def generate_improvement_prompt(self, response: str, metrics: DiscussionMetrics,\n",
        "                                  question: DiscussionQuestion) -> str:\n",
        "        gaps = metrics.knowledge_gaps\n",
        "        prompt_parts = []\n",
        "\n",
        "        if gaps.missing_keywords:\n",
        "            prompt_parts.append(f\"Include these key concepts: {', '.join(gaps.missing_keywords)}\")\n",
        "\n",
        "        if gaps.factual_discrepancies:\n",
        "            prompt_parts.append(\"Correct these factual issues:\\n- \" + \"\\n- \".join(gaps.factual_discrepancies[:3]))\n",
        "\n",
        "        if metrics.logical_consistency < 0.7:\n",
        "            prompt_parts.append(\"Improve logical flow by ensuring all points connect coherently\")\n",
        "\n",
        "        if metrics.completeness_score < 0.7:\n",
        "            missing_criteria = [\n",
        "                crit for crit in question.evaluation_criteria\n",
        "                if not any(kw in response.lower()\n",
        "                          for kw in self._get_criterion_keywords(crit))\n",
        "            ]\n",
        "            if missing_criteria:\n",
        "                prompt_parts.append(\"Address these missing evaluation criteria:\\n- \" + \"\\n- \".join(missing_criteria))\n",
        "\n",
        "        if metrics.semantic_similarity < 0.6:\n",
        "            prompt_parts.append(\"Align more closely with the reference answer's semantic meaning\")\n",
        "\n",
        "        if metrics.collaboration_score < 0.5 and question.perspectives:\n",
        "            missing_perspectives = [\n",
        "                p for p in question.perspectives\n",
        "                if p.lower() not in response.lower()\n",
        "            ]\n",
        "            prompt_parts.append(f\"Consider these additional perspectives: {', '.join(missing_perspectives)}\")\n",
        "\n",
        "        base_prompt = (\n",
        "            f\"Improve this response to '{question.question}':\\n\\n\"\n",
        "            f\"Current response: {response[:500]}\\n\\n\"\n",
        "            \"Specific improvements needed:\\n\"\n",
        "        )\n",
        "\n",
        "        return base_prompt + \"\\n\".join(f\"- {part}\" for part in prompt_parts) if prompt_parts else base_prompt + \"- No specific issues identified\"\n",
        "\n",
        "    def _calculate_factual_accuracy(self, response: str, reference: str,\n",
        "                                  question: DiscussionQuestion) -> float:\n",
        "        response_lower = response.lower()\n",
        "        keyword_score = sum(1 for keyword in question.expected_keywords\n",
        "                         if keyword.lower() in response_lower) / max(len(question.expected_keywords), 1)\n",
        "\n",
        "        numbers_in_response = re.findall(r'\\d+\\.?\\d*', response)\n",
        "        numbers_in_reference = re.findall(r'\\d+\\.?\\d*', reference)\n",
        "\n",
        "        numerical_accuracy = sum(1 for num in numbers_in_reference\n",
        "                               if num in numbers_in_response) / len(numbers_in_reference) if numbers_in_reference else 1.0\n",
        "\n",
        "        return (keyword_score + numerical_accuracy) / 2\n",
        "\n",
        "    def _calculate_logical_consistency(self, response: str) -> float:\n",
        "        sentences = response.split('.')\n",
        "        contradiction_markers = ['however', 'but', 'although', 'despite', 'nevertheless']\n",
        "        contradictions = sum(1 for sentence in sentences\n",
        "                           for marker in contradiction_markers\n",
        "                           if marker in sentence.lower())\n",
        "        return max(0, 1 - (contradictions / max(len(sentences), 1)))\n",
        "\n",
        "    def _calculate_reference_alignment(self, response: str, reference: str) -> float:\n",
        "        try:\n",
        "            docs = [response, reference]\n",
        "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(docs)\n",
        "            return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_coherence(self, response: str) -> float:\n",
        "        sentences = [s.strip() for s in response.split('.') if s.strip()]\n",
        "        if len(sentences) < 2:\n",
        "            return 1.0\n",
        "\n",
        "        transition_words = ['therefore', 'however', 'moreover', 'furthermore',\n",
        "                          'additionally', 'consequently', 'thus', 'hence']\n",
        "        transitions = sum(1 for sentence in sentences[1:]\n",
        "                         for word in transition_words\n",
        "                         if word in sentence.lower())\n",
        "\n",
        "        return min(1.0, transitions / max(len(sentences) - 1, 1) + 0.5)\n",
        "\n",
        "    def _calculate_completeness(self, response: str, question: DiscussionQuestion) -> float:\n",
        "        criteria_coverage = sum(\n",
        "            1 for criterion in question.evaluation_criteria\n",
        "            if any(kw in response.lower()\n",
        "                  for kw in self._get_criterion_keywords(criterion)))\n",
        "        return criteria_coverage / max(len(question.evaluation_criteria), 1)\n",
        "\n",
        "    def _get_criterion_keywords(self, criterion: str) -> List[str]:\n",
        "        keyword_map = {\n",
        "            'factual_accuracy': ['fact', 'accurate', 'correct', 'true'],\n",
        "            'completeness': ['complete', 'comprehensive', 'thorough', 'detailed'],\n",
        "            'logical_consistency': ['logic', 'consistent', 'coherent', 'reasoning'],\n",
        "            'creativity': ['innovative', 'creative', 'novel', 'unique'],\n",
        "            'technical_accuracy': ['technical', 'precise', 'specific', 'accurate'],\n",
        "            'clarity': ['clear', 'understandable', 'simple', 'explain'],\n",
        "            'examples_quality': ['example', 'instance', 'case', 'illustration']\n",
        "        }\n",
        "        return keyword_map.get(criterion, [])\n",
        "\n",
        "    def _calculate_relevance(self, response: str, question: str) -> float:\n",
        "        try:\n",
        "            docs = [response, question]\n",
        "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(docs)\n",
        "            return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_readability(self, text: str) -> float:\n",
        "        sentences = text.split('.')\n",
        "        words = text.split()\n",
        "        if not sentences or not words:\n",
        "            return 0.0\n",
        "\n",
        "        avg_sentence_length = len(words) / len(sentences)\n",
        "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
        "        sentence_score = 1 - abs(avg_sentence_length - 17.5) / 17.5\n",
        "        word_score = 1 - abs(avg_word_length - 5) / 5\n",
        "        return max(0, (sentence_score + word_score) / 2)\n",
        "\n",
        "    def _calculate_grammar_score(self, text: str) -> float:\n",
        "        common_errors = ['teh', 'recieve', 'seperate', 'definately', 'occured']\n",
        "        error_count = sum(1 for error in common_errors if error in text.lower())\n",
        "        sentence_endings = text.count('.') + text.count('!') + text.count('?')\n",
        "        sentences = len([s for s in text.split('.') if s.strip()])\n",
        "        punctuation_score = min(1.0, sentence_endings / max(sentences, 1))\n",
        "        error_penalty = max(0, 1 - error_count * 0.1)\n",
        "        return (punctuation_score + error_penalty) / 2\n",
        "\n",
        "    def _calculate_vocabulary_diversity(self, text: str) -> float:\n",
        "        words = [word.lower() for word in re.findall(r'\\b\\w+\\b', text)]\n",
        "        return len(set(words)) / len(words) if words else 0.0\n",
        "\n",
        "    def _calculate_bleu_score(self, response: str, reference: str) -> float:\n",
        "        try:\n",
        "            response_tokens = word_tokenize(response.lower())\n",
        "            reference_tokens = word_tokenize(reference.lower())\n",
        "            return sentence_bleu([reference_tokens], response_tokens,\n",
        "                               smoothing_function=self.smoothing)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_semantic_similarity(self, response: str, reference: str) -> float:\n",
        "        return self._calculate_reference_alignment(response, reference)\n",
        "\n",
        "    def _calculate_overall_score(self, metrics: DiscussionMetrics) -> float:\n",
        "        weights = {\n",
        "            'factual_accuracy': 0.20,\n",
        "            'logical_consistency': 0.15,\n",
        "            'reference_alignment': 0.15,\n",
        "            'coherence_score': 0.10,\n",
        "            'completeness_score': 0.15,\n",
        "            'relevance_score': 0.10,\n",
        "            'readability_score': 0.05,\n",
        "            'grammar_score': 0.05,\n",
        "            'vocabulary_diversity': 0.05\n",
        "        }\n",
        "        return sum(getattr(metrics, metric_name, 0) * weight\n",
        "                 for metric_name, weight in weights.items())\n",
        "\n",
        "class ModelType(Enum):\n",
        "    GEMMA_3N_E2B = \"tencent/hunyuan-a13b-instruct:free\"\n",
        "    QWEN3_4B = \"mistralai/mistral-small-3.2-24b-instruct:free\"\n",
        "    GEMMA_3_12B = \"z-ai/glm-4.5-air:free\"\n",
        "    KIMI_DEV = \"moonshotai/kimi-dev-72b:free\"\n",
        "    DOLPHIN_MISTRAL = \"cognitivecomputations/dolphin-mistral-24b-venice-edition:free\"\n",
        "\n",
        "class DiscussionRole(Enum):\n",
        "    PERSPECTIVE_ANALYST = \"perspective_analyst\"\n",
        "    KNOWLEDGE_INTEGRATOR = \"knowledge_integrator\"\n",
        "    SYNTHESIZER = \"synthesizer\"\n",
        "    FACT_CHECKER = \"fact_checker\"\n",
        "    CLARIFIER = \"clarifier\"\n",
        "\n",
        "@dataclass\n",
        "class ModelResponse:\n",
        "    model_name: str\n",
        "    answer: str\n",
        "    generation_time: float\n",
        "    token_count: int = 0\n",
        "    discussion_role: DiscussionRole = DiscussionRole.PERSPECTIVE_ANALYST\n",
        "    reasoning_chain: List[str] = None\n",
        "    error: Optional[str] = None\n",
        "    discussion_metrics: Optional[DiscussionMetrics] = None\n",
        "\n",
        "@dataclass\n",
        "class DiscussionRound:\n",
        "    round_number: int\n",
        "    contributions: List[str]\n",
        "    perspectives_covered: List[str]\n",
        "    synthesis: Optional[str] = None\n",
        "    contribution_metrics: Dict[str, DiscussionMetrics] = field(default_factory=dict)\n",
        "    synthesis_metrics: Optional[DiscussionMetrics] = None\n",
        "    role_assignments: Dict[str, str] = field(default_factory=dict)\n",
        "    improvement_scores: Dict[str, float] = field(default_factory=dict)\n",
        "    discussion_metrics: Dict[str, float] = field(default_factory=dict)\n",
        "    questions_raised: List[str] = field(default_factory=list)\n",
        "\n",
        "@dataclass\n",
        "class DiscussionMemory:\n",
        "    history: Deque[DiscussionRound]\n",
        "    current_consensus: Optional[str] = None\n",
        "    knowledge_gaps: List[str] = None\n",
        "    open_questions: List[str] = None\n",
        "\n",
        "class OpenRouterClient:\n",
        "    def __init__(self, api_key: str, base_url: str = \"https://openrouter.ai/api/v1\"):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = base_url\n",
        "        self.session = None\n",
        "        self.timeout = aiohttp.ClientTimeout(total=90, connect=15, sock_read=60)\n",
        "        self.max_retries = 3\n",
        "        self.base_delay = 2\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        logger.info(\"Initializing OpenRouter client session...\")\n",
        "        connector = aiohttp.TCPConnector(\n",
        "            limit=10,\n",
        "            ttl_dns_cache=300,\n",
        "            use_dns_cache=True,\n",
        "            keepalive_timeout=60,\n",
        "            enable_cleanup_closed=True\n",
        "        )\n",
        "        self.session = aiohttp.ClientSession(\n",
        "            timeout=self.timeout,\n",
        "            connector=connector\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
        "        if self.session:\n",
        "            logger.info(\"Closing OpenRouter client session...\")\n",
        "            await self.session.close()\n",
        "\n",
        "    async def generate_response(self, model: str, messages: List[Dict],\n",
        "                               temperature: float = 0.7, max_tokens: int = 1500) -> Dict:\n",
        "        logger.info(f\"Generating response from model: {model}\")\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"HTTP-Referer\": \"https://your-app.com\",\n",
        "            \"X-Title\": \"Enhanced Discussion System\",\n",
        "            \"User-Agent\": \"Enhanced-Discussion-System/1.0\"\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"stream\": False\n",
        "        }\n",
        "\n",
        "        for attempt in range(self.max_retries):\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                if attempt > 0:\n",
        "                    delay = self.base_delay * (2 ** (attempt - 1)) + random.uniform(0, 1)\n",
        "                    logger.info(f\"Retrying {model} after {delay:.2f}s (attempt {attempt + 1}/{self.max_retries})\")\n",
        "                    await asyncio.sleep(delay)\n",
        "\n",
        "                async with self.session.post(\n",
        "                    f\"{self.base_url}/chat/completions\",\n",
        "                    headers=headers,\n",
        "                    json=payload\n",
        "                ) as response:\n",
        "                    if response.status == 429:\n",
        "                        retry_after = int(response.headers.get('Retry-After', 10))\n",
        "                        logger.warning(f\"Rate limited. Waiting {retry_after} seconds...\")\n",
        "                        await asyncio.sleep(retry_after)\n",
        "                        continue\n",
        "\n",
        "                    if 500 <= response.status < 600:\n",
        "                        logger.warning(f\"Server error {response.status} for {model}, will retry\")\n",
        "                        continue\n",
        "\n",
        "                    if 400 <= response.status < 500 and response.status != 408:\n",
        "                        error_text = await response.text()\n",
        "                        logger.error(f\"Client error {response.status} for {model}: {error_text}\")\n",
        "                        return {\n",
        "                            \"error\": f\"HTTP {response.status}: {error_text[:200]}...\",\n",
        "                            \"status\": response.status\n",
        "                        }\n",
        "\n",
        "                    if response.status == 408:\n",
        "                        logger.warning(f\"Request timeout (408) for {model}, will retry\")\n",
        "                        continue\n",
        "\n",
        "                    response.raise_for_status()\n",
        "                    result = await response.json()\n",
        "\n",
        "                    return {\n",
        "                        \"content\": result[\"choices\"][0][\"message\"][\"content\"],\n",
        "                        \"response_time\": time.time() - start_time,\n",
        "                        \"usage\": result.get(\"usage\", {}),\n",
        "                        \"tokens\": result.get(\"usage\", {}).get(\"total_tokens\", 0),\n",
        "                        \"model\": model,\n",
        "                        \"attempt\": attempt + 1\n",
        "                    }\n",
        "\n",
        "            except asyncio.TimeoutError:\n",
        "                logger.warning(f\"Timeout for {model} on attempt {attempt + 1}\")\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    return {\n",
        "                        \"error\": f\"Timeout after {self.max_retries} attempts\",\n",
        "                        \"status\": 408\n",
        "                    }\n",
        "                continue\n",
        "\n",
        "            except aiohttp.ClientError as e:\n",
        "                logger.error(f\"HTTP client error with model {model}: {str(e)}\")\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    return {\n",
        "                        \"error\": f\"HTTP client error: {str(e)}\",\n",
        "                        \"status\": getattr(e, \"status\", None)\n",
        "                    }\n",
        "                continue\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Unexpected error with model {model}: {str(e)}\")\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    return {\n",
        "                        \"error\": f\"Unexpected error: {str(e)}\"\n",
        "                    }\n",
        "                continue\n",
        "\n",
        "        return {\n",
        "            \"error\": f\"Failed after {self.max_retries} attempts\",\n",
        "            \"status\": None\n",
        "        }\n",
        "\n",
        "class DiscussionSystem:\n",
        "    def __init__(self):\n",
        "        self.memory = DiscussionMemory(\n",
        "            history=deque(maxlen=3),\n",
        "            knowledge_gaps=[],\n",
        "            open_questions=[]\n",
        "        )\n",
        "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
        "        self.evaluation_system = AdvancedEvaluationSystem()\n",
        "        self.benchmark_dataset = DiscussionDataset.get_comprehensive_dataset()\n",
        "        self.regeneration_attempts = defaultdict(int)\n",
        "        self.MAX_REGENERATIONS = 2\n",
        "\n",
        "    async def conduct_discussion(self, query: str, initial_responses: List[ModelResponse]) -> DiscussionRound:\n",
        "        current_round = DiscussionRound(\n",
        "            round_number=len(self.memory.history) + 1,\n",
        "            contributions=[],\n",
        "            perspectives_covered=[]\n",
        "        )\n",
        "\n",
        "        benchmark_question = next(\n",
        "            (q for q in self.benchmark_dataset if q.question.lower() in query.lower()),\n",
        "            None\n",
        "        )\n",
        "\n",
        "        evaluated_responses = []\n",
        "        for response in initial_responses:\n",
        "            if not response.error:\n",
        "                response.discussion_metrics = self.evaluation_system.calculate_comprehensive_metrics(\n",
        "                    response.answer,\n",
        "                    benchmark_question.reference_answer if benchmark_question else \"\",\n",
        "                    benchmark_question if benchmark_question else DiscussionQuestion(\n",
        "                        id=\"custom\",\n",
        "                        question=query,\n",
        "                        category=\"custom\",\n",
        "                        difficulty=\"medium\",\n",
        "                        reference_answer=\"\",\n",
        "                        evaluation_criteria=[]\n",
        "                    ),\n",
        "                    response.generation_time\n",
        "                )\n",
        "                evaluated_responses.append(response)\n",
        "\n",
        "        regenerated_responses = await self._perform_targeted_regeneration(\n",
        "            evaluated_responses, query, benchmark_question\n",
        "        )\n",
        "\n",
        "        all_responses = evaluated_responses + regenerated_responses\n",
        "        valid_responses = [r for r in all_responses if not r.error]\n",
        "        if not valid_responses:\n",
        "            logger.error(\"No valid responses for discussion\")\n",
        "            return current_round\n",
        "\n",
        "        roles_assigned = self._assign_roles_with_explanations(valid_responses, benchmark_question)\n",
        "\n",
        "        current_round.role_assignments = {\n",
        "            response.model_name: {\n",
        "                'role': response.discussion_role.value,\n",
        "                'reason': f\"Assigned {response.discussion_role.value} role due to benchmark score of {response.discussion_metrics.overall_score:.2f}\"\n",
        "                          if response.discussion_metrics else \"Assigned based on response characteristics\"\n",
        "            }\n",
        "            for response in roles_assigned\n",
        "        }\n",
        "\n",
        "        discussion_tasks = []\n",
        "        for response in roles_assigned:\n",
        "            if response.discussion_role == DiscussionRole.PERSPECTIVE_ANALYST and benchmark_question:\n",
        "                task = self._analyze_perspective(\n",
        "                    response, query, benchmark_question\n",
        "                )\n",
        "            elif response.discussion_role == DiscussionRole.KNOWLEDGE_INTEGRATOR:\n",
        "                task = self._integrate_knowledge(\n",
        "                    response, query, valid_responses, benchmark_question\n",
        "                )\n",
        "            elif response.discussion_role == DiscussionRole.CLARIFIER:\n",
        "                task = self._generate_clarifying_questions(\n",
        "                    response, query, valid_responses, benchmark_question\n",
        "                )\n",
        "            else:\n",
        "                continue\n",
        "            discussion_tasks.append(task)\n",
        "\n",
        "        try:\n",
        "            results = await asyncio.gather(*discussion_tasks, return_exceptions=True)\n",
        "\n",
        "            for result in results:\n",
        "                if isinstance(result, Exception):\n",
        "                    logger.error(f\"Discussion task failed: {str(result)}\")\n",
        "                    continue\n",
        "\n",
        "                if 'contribution' in result:\n",
        "                    current_round.contributions.append(result['contribution'])\n",
        "                if 'questions' in result:\n",
        "                    current_round.questions_raised.extend(result['questions'])\n",
        "                if 'perspectives' in result and benchmark_question:\n",
        "                    current_round.perspectives_covered.extend(result['perspectives'])\n",
        "\n",
        "            if benchmark_question:\n",
        "                self._evaluate_contributions(current_round, benchmark_question)\n",
        "\n",
        "                synthesizer = next(\n",
        "                    (r for r in roles_assigned if r.discussion_role == DiscussionRole.SYNTHESIZER),\n",
        "                    None\n",
        "                )\n",
        "                if synthesizer:\n",
        "                    await self._synthesize_discussion(\n",
        "                        current_round, synthesizer, query, benchmark_question\n",
        "                    )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Discussion round failed: {str(e)}\")\n",
        "\n",
        "        self.memory.history.append(current_round)\n",
        "        return current_round\n",
        "\n",
        "    async def _perform_targeted_regeneration(self, responses: List[ModelResponse],\n",
        "                                           query: str,\n",
        "                                           benchmark_question: Optional[DiscussionQuestion]\n",
        "                                          ) -> List[ModelResponse]:\n",
        "        regenerated = []\n",
        "\n",
        "        if not benchmark_question:\n",
        "            return regenerated\n",
        "\n",
        "        for response in responses:\n",
        "            if (response.discussion_metrics and\n",
        "                response.discussion_metrics.overall_score < 0.6 and\n",
        "                self.regeneration_attempts[response.model_name] < self.MAX_REGENERATIONS):\n",
        "\n",
        "                logger.info(f\"Attempting regeneration for {response.model_name} (score: {response.discussion_metrics.overall_score:.2f})\")\n",
        "\n",
        "                prompt = self.evaluation_system.generate_improvement_prompt(\n",
        "                    response.answer,\n",
        "                    response.discussion_metrics,\n",
        "                    benchmark_question\n",
        "                )\n",
        "\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": \"Please improve your previous response based on the feedback.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "\n",
        "                try:\n",
        "                    async with OpenRouterClient(\"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\") as client:\n",
        "                        result = await client.generate_response(\n",
        "                            response.model_name,\n",
        "                            messages,\n",
        "                            temperature=0.5\n",
        "                        )\n",
        "\n",
        "                        if result and \"error\" not in result:\n",
        "                            new_metrics = self.evaluation_system.calculate_comprehensive_metrics(\n",
        "                                result[\"content\"],\n",
        "                                benchmark_question.reference_answer,\n",
        "                                benchmark_question,\n",
        "                                result[\"response_time\"]\n",
        "                            )\n",
        "\n",
        "                            if (new_metrics.overall_score >\n",
        "                                response.discussion_metrics.overall_score + 0.1):\n",
        "                                regenerated.append(ModelResponse(\n",
        "                                    model_name=response.model_name,\n",
        "                                    answer=result[\"content\"],\n",
        "                                    generation_time=result[\"response_time\"],\n",
        "                                    token_count=result.get(\"tokens\", 0),\n",
        "                                    discussion_metrics=new_metrics,\n",
        "                                    discussion_role=response.discussion_role\n",
        "                                ))\n",
        "                                self.regeneration_attempts[response.model_name] += 1\n",
        "                                logger.info(f\"Regeneration improved score from {response.discussion_metrics.overall_score:.2f} to {new_metrics.overall_score:.2f}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Regeneration failed for {response.model_name}: {str(e)}\")\n",
        "\n",
        "        return regenerated\n",
        "\n",
        "    def _evaluate_contributions(self, discussion_round: DiscussionRound,\n",
        "                              benchmark_question: DiscussionQuestion):\n",
        "        for i, contribution in enumerate(discussion_round.contributions):\n",
        "            discussion_round.contribution_metrics[f\"contribution_{i}\"] = (\n",
        "                self.evaluation_system.calculate_comprehensive_metrics(\n",
        "                    contribution,\n",
        "                    benchmark_question.reference_answer,\n",
        "                    benchmark_question,\n",
        "                    0\n",
        "                )\n",
        "            )\n",
        "\n",
        "    async def _synthesize_discussion(self, discussion_round: DiscussionRound,\n",
        "                                   synthesizer: ModelResponse, query: str,\n",
        "                                   benchmark_question: DiscussionQuestion):\n",
        "        try:\n",
        "            prompt = self._create_synthesis_prompt(\n",
        "                query,\n",
        "                discussion_round.contributions,\n",
        "                discussion_round.contribution_metrics,\n",
        "                benchmark_question,\n",
        "                discussion_round.questions_raised\n",
        "            )\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"Synthesize the discussion by combining insights and filling knowledge gaps.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            async with OpenRouterClient(\"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\") as client:\n",
        "                result = await client.generate_response(\n",
        "                    synthesizer.model_name,\n",
        "                    messages,\n",
        "                    temperature=0.3\n",
        "                )\n",
        "\n",
        "                if result and \"error\" not in result:\n",
        "                    discussion_round.synthesis = result[\"content\"]\n",
        "                    self.memory.current_consensus = result[\"content\"]\n",
        "\n",
        "                    synthesis_metrics = self.evaluation_system.calculate_comprehensive_metrics(\n",
        "                        result[\"content\"],\n",
        "                        benchmark_question.reference_answer,\n",
        "                        benchmark_question,\n",
        "                        result[\"response_time\"]\n",
        "                    )\n",
        "                    discussion_round.synthesis_metrics = synthesis_metrics\n",
        "\n",
        "                    discussion_round.discussion_metrics = self._calculate_discussion_metrics(\n",
        "                        discussion_round.contributions,\n",
        "                        result[\"content\"],\n",
        "                        benchmark_question\n",
        "                    )\n",
        "\n",
        "                    initial_scores = [\n",
        "                        m.overall_score for m in\n",
        "                        discussion_round.contribution_metrics.values()\n",
        "                    ]\n",
        "                    if initial_scores:\n",
        "                        discussion_round.improvement_scores = {\n",
        "                            'initial_avg': statistics.mean(initial_scores),\n",
        "                            'synthesis_score': synthesis_metrics.overall_score,\n",
        "                            'improvement': (synthesis_metrics.overall_score -\n",
        "                                          statistics.mean(initial_scores))\n",
        "                        }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Synthesis failed: {str(e)}\")\n",
        "            discussion_round.synthesis = \"Failed to generate synthesis\"\n",
        "\n",
        "    def _calculate_discussion_metrics(self, contributions: List[str],\n",
        "                                    synthesis: str, question: DiscussionQuestion) -> Dict[str, float]:\n",
        "        metrics = {}\n",
        "        contribution_scores = []\n",
        "        for contribution in contributions:\n",
        "            cont_metrics = self.evaluation_system.calculate_comprehensive_metrics(\n",
        "                contribution, question.reference_answer, question, 0\n",
        "            )\n",
        "            contribution_scores.append(cont_metrics.overall_score)\n",
        "        metrics['contribution_quality'] = statistics.mean(contribution_scores) if contribution_scores else 0\n",
        "\n",
        "        if synthesis:\n",
        "            synth_metrics = self.evaluation_system.calculate_comprehensive_metrics(\n",
        "                synthesis, question.reference_answer, question, 0\n",
        "            )\n",
        "            metrics['synthesis_quality'] = synth_metrics.overall_score\n",
        "            metrics['perspective_coverage'] = synth_metrics.collaboration_score\n",
        "        else:\n",
        "            metrics['synthesis_quality'] = 0\n",
        "            metrics['perspective_coverage'] = 0\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _create_synthesis_prompt(self, query: str, contributions: List[str],\n",
        "                               contribution_metrics: Dict[str, DiscussionMetrics],\n",
        "                               benchmark_question: DiscussionQuestion,\n",
        "                               questions: List[str]) -> str:\n",
        "\n",
        "        questions_text = (\n",
        "    \"Questions raised during discussion:\\n- \" + \"\\n- \".join(questions)\n",
        "    if questions else \"No questions were raised\"\n",
        ")\n",
        "\n",
        "        perspectives_text = (\n",
        "            \"Available perspectives to consider:\\n- \" + \"\\n- \".join(benchmark_question.perspectives)\n",
        "            if benchmark_question.perspectives else \"\"\n",
        "        )\n",
        "        prompt = f'''\n",
        "Synthesize the discussion on: {query}\n",
        "\n",
        "Key contributions:\n",
        "{self._format_contributions(contributions, contribution_metrics)}\n",
        "\n",
        "{questions_text}\n",
        "\n",
        "{perspectives_text}\n",
        "\n",
        "Guidelines for synthesis:\n",
        "1. Combine the most valuable insights from all contributions\n",
        "2. Address any knowledge gaps identified\n",
        "3. Maintain factual accuracy\n",
        "4. Acknowledge different perspectives\n",
        "5. Note any remaining uncertainties or open questions\n",
        "6. Provide a clear, coherent summary\n",
        "'''\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def _format_contributions(self, contributions: List[str],\n",
        "                         metrics: Dict[str, DiscussionMetrics],\n",
        "                         prefix: str = \"contribution\") -> str:\n",
        "        formatted = []\n",
        "        for i, cont in enumerate(contributions):\n",
        "            metric_key = f\"{prefix}_{i}\"\n",
        "            if metric_key in metrics:\n",
        "                score = metrics[metric_key].overall_score\n",
        "                gaps = \", \".join(metrics[metric_key].knowledge_gaps.missing_keywords)\n",
        "                formatted.append(\n",
        "                    f\"- [Score: {score:.2f}] {cont[:200]}...\\n\"\n",
        "                    f\"  Missing concepts: {gaps or 'None'}\"\n",
        "                )\n",
        "            else:\n",
        "                formatted.append(f\"- {cont[:200]}...\")\n",
        "        return \"\\n\".join(formatted)\n",
        "\n",
        "    async def _analyze_perspective(self, model: ModelResponse, query: str,\n",
        "                                 question: DiscussionQuestion) -> Dict:\n",
        "        try:\n",
        "            perspective = random.choice(question.perspectives) if question.perspectives else \"general\"\n",
        "            prompt = f\"Analyze the following question from a {perspective} perspective:\\n\\n{query}\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are analyzing a question from a specific perspective.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            async with OpenRouterClient(\"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\") as client:\n",
        "                result = await client.generate_response(\n",
        "                    model.model_name,\n",
        "                    messages,\n",
        "                    temperature=0.7\n",
        "                )\n",
        "\n",
        "                if result and \"error\" not in result:\n",
        "                    return {\n",
        "                        'contribution': result[\"content\"],\n",
        "                        'perspectives': [perspective]\n",
        "                    }\n",
        "                return {\n",
        "                    'contribution': f\"Failed to generate perspective analysis ({result.get('error', 'unknown')})\",\n",
        "                    'perspectives': []\n",
        "                }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to analyze perspective: {str(e)}\")\n",
        "            return {'contribution': '', 'perspectives': []}\n",
        "\n",
        "    async def _integrate_knowledge(self, model: ModelResponse, query: str,\n",
        "                                responses: List[ModelResponse],\n",
        "                                question: Optional[DiscussionQuestion]) -> Dict:\n",
        "        try:\n",
        "            context = \"\\n\\n\".join([r.answer[:500] for r in responses[:3]])\n",
        "            prompt = f\"\"\"Integrate knowledge from multiple sources to address:\n",
        "{query}\n",
        "\n",
        "Available information:\n",
        "{context}\n",
        "\n",
        "Guidelines:\n",
        "1. Combine the most valuable insights\n",
        "2. Resolve any contradictions\n",
        "3. Fill knowledge gaps where possible\n",
        "4. Maintain a neutral, objective tone\"\"\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are integrating knowledge from multiple sources.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            async with OpenRouterClient(\"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\") as client:\n",
        "                result = await client.generate_response(\n",
        "                    model.model_name,\n",
        "                    messages,\n",
        "                    temperature=0.5\n",
        "                )\n",
        "\n",
        "                if result and \"error\" not in result:\n",
        "                    return {\n",
        "                        'contribution': result[\"content\"]\n",
        "                    }\n",
        "                return {\n",
        "                    'contribution': f\"Failed to integrate knowledge ({result.get('error', 'unknown')})\"\n",
        "                }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to integrate knowledge: {str(e)}\")\n",
        "            return {'contribution': ''}\n",
        "\n",
        "    async def _generate_clarifying_questions(self, model: ModelResponse, query: str,\n",
        "                                          responses: List[ModelResponse],\n",
        "                                          question: Optional[DiscussionQuestion]) -> Dict:\n",
        "        try:\n",
        "            context = \"\\n\".join([r.answer[:300] for r in responses[:3]])\n",
        "            prompt = f\"\"\"Based on the following discussion about: {query}\n",
        "\n",
        "Current contributions:\n",
        "{context}\n",
        "\n",
        "Generate 2-3 clarifying questions that would help improve the discussion by:\n",
        "1. Identifying missing information\n",
        "2. Resolving contradictions\n",
        "3. Exploring alternative perspectives\n",
        "4. Deepening the analysis\"\"\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You generate clarifying questions to improve discussions.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            async with OpenRouterClient(\"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\") as client:\n",
        "                result = await client.generate_response(\n",
        "                    model.model_name,\n",
        "                    messages,\n",
        "                    temperature=0.7\n",
        "                )\n",
        "\n",
        "                if result and \"error\" not in result:\n",
        "                    questions = [q.strip() for q in result[\"content\"].split('\\n') if q.strip()]\n",
        "                    return {\n",
        "                        'questions': questions[:3]  # Limit to 3 questions\n",
        "                    }\n",
        "                return {\n",
        "                    'questions': [f\"Failed to generate questions ({result.get('error', 'unknown')})\"]\n",
        "                }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate questions: {str(e)}\")\n",
        "            return {'questions': []}\n",
        "\n",
        "    def _assign_roles_with_explanations(self, responses: List[ModelResponse],\n",
        "                                      question: Optional[DiscussionQuestion]) -> List[ModelResponse]:\n",
        "        if not responses:\n",
        "            return []\n",
        "\n",
        "        # Sort responses by quality metrics if available\n",
        "        sorted_responses = sorted(\n",
        "            responses,\n",
        "            key=lambda x: (\n",
        "                -x.discussion_metrics.overall_score if x.discussion_metrics else 0,\n",
        "                len(x.answer)\n",
        "            ),\n",
        "            reverse=True\n",
        "        )[:5]  # Consider top 5 responses for roles\n",
        "\n",
        "        print(\"\\nRole Assignments:\")\n",
        "        assigned_roles = set()\n",
        "\n",
        "        for i, response in enumerate(sorted_responses):\n",
        "            score = response.discussion_metrics.overall_score if response.discussion_metrics else \"N/A\"\n",
        "            print(f\"Model: {response.model_name}\")\n",
        "            print(f\"- Benchmark Score: {score}\")\n",
        "            print(f\"- Answer Length: {len(response.answer)} chars\")\n",
        "\n",
        "            # Assign roles based on response characteristics\n",
        "            if DiscussionRole.KNOWLEDGE_INTEGRATOR not in assigned_roles:\n",
        "                role = DiscussionRole.KNOWLEDGE_INTEGRATOR\n",
        "                assigned_roles.add(role)\n",
        "                print(\"- Role: Knowledge Integrator (best at combining information)\")\n",
        "            elif question and question.perspectives and DiscussionRole.PERSPECTIVE_ANALYST not in assigned_roles:\n",
        "                role = DiscussionRole.PERSPECTIVE_ANALYST\n",
        "                assigned_roles.add(role)\n",
        "                print(\"- Role: Perspective Analyst (will explore different viewpoints)\")\n",
        "            elif DiscussionRole.CLARIFIER not in assigned_roles:\n",
        "                role = DiscussionRole.CLARIFIER\n",
        "                assigned_roles.add(role)\n",
        "                print(\"- Role: Clarifier (will generate probing questions)\")\n",
        "            elif DiscussionRole.SYNTHESIZER not in assigned_roles:\n",
        "                role = DiscussionRole.SYNTHESIZER\n",
        "                assigned_roles.add(role)\n",
        "                print(\"- Role: Synthesizer (will create final summary)\")\n",
        "            else:\n",
        "                role = DiscussionRole.PERSPECTIVE_ANALYST\n",
        "                print(\"- Role: Perspective Analyst (default role)\")\n",
        "\n",
        "            response.discussion_role = role\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        return sorted_responses[:len(assigned_roles)]  # Return only responses with assigned roles\n",
        "\n",
        "class EnhancedMultiJudgeSystem:\n",
        "    def __init__(self, openrouter_api_key: str):\n",
        "        self.client = OpenRouterClient(openrouter_api_key)\n",
        "        self.models = [model.value for model in ModelType]\n",
        "        self.discussion_system = DiscussionSystem()\n",
        "        self.evaluation_system = AdvancedEvaluationSystem()\n",
        "        self.benchmark_dataset = DiscussionDataset.get_comprehensive_dataset()\n",
        "        self.INITIAL_RESPONSE_TIMEOUT = 120\n",
        "        self.DISCUSSION_TIMEOUT = 180\n",
        "        self.MODEL_TIMEOUT = 45\n",
        "\n",
        "    async def evaluate_single_question(self, question_text: str) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        result = {\n",
        "            \"final_answer\": \"\",\n",
        "            \"processing_time\": 0,\n",
        "            \"errors\": [],\n",
        "            \"models_responded\": 0,\n",
        "            \"models_failed\": 0,\n",
        "            \"benchmark_scores\": {},\n",
        "            \"discussion_metrics\": {},\n",
        "            \"improvement_scores\": {},\n",
        "            \"role_assignments\": {},\n",
        "            \"contribution_metrics\": {},\n",
        "            \"questions_raised\": []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            try:\n",
        "                responses = await asyncio.wait_for(\n",
        "                    self.generate_all_responses(question_text),\n",
        "                    timeout=self.INITIAL_RESPONSE_TIMEOUT\n",
        "                )\n",
        "            except asyncio.TimeoutError:\n",
        "                raise Exception(f\"Timeout generating initial responses after {self.INITIAL_RESPONSE_TIMEOUT}s\")\n",
        "\n",
        "            valid_responses = [r for r in responses if not r.error]\n",
        "            result[\"models_responded\"] = len(valid_responses)\n",
        "            result[\"models_failed\"] = len(responses) - len(valid_responses)\n",
        "\n",
        "            if not valid_responses:\n",
        "                raise Exception(\"All models failed to respond\")\n",
        "\n",
        "            benchmark_question = next(\n",
        "                (q for q in self.benchmark_dataset if q.question.lower() in question_text.lower()),\n",
        "                None\n",
        "            )\n",
        "\n",
        "            result[\"benchmark_scores\"] = {\n",
        "                r.model_name: r.discussion_metrics.overall_score\n",
        "                for r in valid_responses\n",
        "                if r.discussion_metrics\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                discussion_round = await asyncio.wait_for(\n",
        "                    self.discussion_system.conduct_discussion(question_text, valid_responses),\n",
        "                    timeout=self.DISCUSSION_TIMEOUT\n",
        "                )\n",
        "\n",
        "                result.update({\n",
        "                    \"final_answer\": discussion_round.synthesis or \"No synthesis generated\",\n",
        "                    \"discussion_metrics\": getattr(discussion_round, 'discussion_metrics', {}),\n",
        "                    \"improvement_scores\": getattr(discussion_round, 'improvement_scores', {}),\n",
        "                    \"role_assignments\": getattr(discussion_round, 'role_assignments', {}),\n",
        "                    \"contribution_metrics\": getattr(discussion_round, 'contribution_metrics', {}),\n",
        "                    \"questions_raised\": getattr(discussion_round, 'questions_raised', []),\n",
        "                    \"discussion_rounds\": len(self.discussion_system.memory.history)\n",
        "                })\n",
        "\n",
        "            except asyncio.TimeoutError:\n",
        "                raise Exception(f\"Timeout during discussion after {self.DISCUSSION_TIMEOUT}s\")\n",
        "\n",
        "            result.update({\n",
        "                \"processing_time\": time.time() - start_time,\n",
        "                \"success\": True\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Processing failed: {str(e)}\")\n",
        "            result.update({\n",
        "                \"errors\": [str(e)],\n",
        "                \"processing_time\": time.time() - start_time,\n",
        "                \"success\": False\n",
        "            })\n",
        "\n",
        "        return result\n",
        "\n",
        "    async def generate_all_responses(self, user_query: str) -> List[ModelResponse]:\n",
        "        responses = []\n",
        "\n",
        "        async with self.client:\n",
        "            tasks = []\n",
        "            for model in self.models:\n",
        "                messages = self._create_discussion_prompt(user_query)\n",
        "                task = asyncio.wait_for(\n",
        "                    self.client.generate_response(model, messages),\n",
        "                    timeout=self.MODEL_TIMEOUT\n",
        "                )\n",
        "                tasks.append((model, task))\n",
        "\n",
        "            for model, task in tasks:\n",
        "                try:\n",
        "                    result = await task\n",
        "                    if result and \"error\" not in result:\n",
        "                        benchmark_question = next(\n",
        "                            (q for q in self.benchmark_dataset\n",
        "                             if q.question.lower() in user_query.lower()),\n",
        "                            None\n",
        "                        )\n",
        "\n",
        "                        metrics = None\n",
        "                        if benchmark_question:\n",
        "                            metrics = self.evaluation_system.calculate_comprehensive_metrics(\n",
        "                                result[\"content\"],\n",
        "                                benchmark_question.reference_answer,\n",
        "                                benchmark_question,\n",
        "                                result[\"response_time\"]\n",
        "                            )\n",
        "\n",
        "                        responses.append(ModelResponse(\n",
        "                            model_name=model,\n",
        "                            answer=result[\"content\"],\n",
        "                            generation_time=result[\"response_time\"],\n",
        "                            token_count=result.get(\"tokens\", 0),\n",
        "                            discussion_metrics=metrics\n",
        "                        ))\n",
        "                    else:\n",
        "                        error_msg = result.get(\"error\", \"Unknown error\") if result else \"No response\"\n",
        "                        responses.append(ModelResponse(\n",
        "                            model_name=model,\n",
        "                            answer=\"\",\n",
        "                            generation_time=0,\n",
        "                            error=error_msg\n",
        "                        ))\n",
        "                except asyncio.TimeoutError:\n",
        "                    responses.append(ModelResponse(\n",
        "                        model_name=model,\n",
        "                        answer=\"\",\n",
        "                        generation_time=0,\n",
        "                        error=f\"Timeout after {self.MODEL_TIMEOUT}s\"\n",
        "                    ))\n",
        "                except Exception as e:\n",
        "                    responses.append(ModelResponse(\n",
        "                        model_name=model,\n",
        "                        answer=\"\",\n",
        "                        generation_time=0,\n",
        "                        error=str(e)\n",
        "                    ))\n",
        "\n",
        "        return responses\n",
        "\n",
        "    def _create_discussion_prompt(self, query: str) -> List[Dict]:\n",
        "        return [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are participating in a collaborative discussion. Provide thoughtful, well-reasoned contributions.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"{query}\\n\\nPresent your perspective with supporting evidence in a discussion-friendly format.\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    async def process_query(self, user_query: str) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        result = {\n",
        "            \"final_answer\": \"\",\n",
        "            \"processing_time\": 0,\n",
        "            \"errors\": [],\n",
        "            \"models_responded\": 0,\n",
        "            \"models_failed\": 0,\n",
        "            \"benchmark_scores\": {},\n",
        "            \"discussion_metrics\": {},\n",
        "            \"improvement_scores\": {},\n",
        "            \"role_assignments\": {},\n",
        "            \"contribution_metrics\": {},\n",
        "            \"questions_raised\": []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            try:\n",
        "                responses = await asyncio.wait_for(\n",
        "                    self.generate_all_responses(user_query),\n",
        "                    timeout=self.INITIAL_RESPONSE_TIMEOUT\n",
        "                )\n",
        "            except asyncio.TimeoutError:\n",
        "                raise Exception(f\"Timeout while generating initial responses after {self.INITIAL_RESPONSE_TIMEOUT}s\")\n",
        "\n",
        "            valid_responses = [r for r in responses if not r.error]\n",
        "            result[\"models_responded\"] = len(valid_responses)\n",
        "            result[\"models_failed\"] = len(responses) - len(valid_responses)\n",
        "\n",
        "            if not valid_responses:\n",
        "                raise Exception(\"All models failed to respond\")\n",
        "\n",
        "            benchmark_question = next(\n",
        "                (q for q in self.benchmark_dataset if q.question.lower() in user_query.lower()),\n",
        "                None\n",
        "            )\n",
        "\n",
        "            result[\"benchmark_scores\"] = {\n",
        "                r.model_name: r.discussion_metrics.overall_score\n",
        "                for r in valid_responses\n",
        "                if r.discussion_metrics\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                discussion_round = await asyncio.wait_for(\n",
        "                    self.discussion_system.conduct_discussion(user_query, valid_responses),\n",
        "                    timeout=self.DISCUSSION_TIMEOUT\n",
        "                )\n",
        "\n",
        "                result.update({\n",
        "                    \"final_answer\": discussion_round.synthesis or \"No synthesis generated\",\n",
        "                    \"discussion_metrics\": getattr(discussion_round, 'discussion_metrics', {}),\n",
        "                    \"improvement_scores\": getattr(discussion_round, 'improvement_scores', {}),\n",
        "                    \"role_assignments\": getattr(discussion_round, 'role_assignments', {}),\n",
        "                    \"contribution_metrics\": getattr(discussion_round, 'contribution_metrics', {}),\n",
        "                    \"questions_raised\": getattr(discussion_round, 'questions_raised', []),\n",
        "                    \"discussion_rounds\": len(self.discussion_system.memory.history)\n",
        "                })\n",
        "\n",
        "            except asyncio.TimeoutError:\n",
        "                raise Exception(f\"Timeout during discussion after {self.DISCUSSION_TIMEOUT}s\")\n",
        "\n",
        "            result.update({\n",
        "                \"processing_time\": time.time() - start_time,\n",
        "                \"success\": True\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Processing failed: {str(e)}\")\n",
        "            result.update({\n",
        "                \"errors\": [str(e)],\n",
        "                \"processing_time\": time.time() - start_time,\n",
        "                \"success\": False\n",
        "            })\n",
        "\n",
        "        return result\n",
        "\n",
        "async def main():\n",
        "    API_KEY = \"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\"  # Replace with your actual API key\n",
        "\n",
        "    try:\n",
        "        system = EnhancedMultiJudgeSystem(API_KEY)\n",
        "\n",
        "        # Evaluate the ethics question in detail\n",
        "        print(\"Evaluating ethics question in detail...\")\n",
        "        question = next(q for q in system.benchmark_dataset if q.id == \"ethics_001\")\n",
        "        results = await system.evaluate_single_question(question.question)\n",
        "\n",
        "        print(\"\\nDetailed Results:\")\n",
        "        print(f\"Question: {question.question}\")\n",
        "        print(f\"Category: {question.category}\")\n",
        "        print(f\"Difficulty: {question.difficulty}\")\n",
        "\n",
        "        print(\"\\nModels Responded:\", results[\"models_responded\"])\n",
        "        print(\"Models Failed:\", results[\"models_failed\"])\n",
        "\n",
        "        print(\"\\nBenchmark Scores:\")\n",
        "        for model, score in results[\"benchmark_scores\"].items():\n",
        "            print(f\"- {model}: {score:.3f}\")\n",
        "\n",
        "        print(\"\\nRole Assignments:\")\n",
        "        for model, info in results[\"role_assignments\"].items():\n",
        "            print(f\"- {model}: {info['role']} ({info['reason']})\")\n",
        "\n",
        "        print(\"\\nQuestions Raised:\")\n",
        "        for q in results[\"questions_raised\"]:\n",
        "            print(f\"- {q}\")\n",
        "\n",
        "        print(\"\\nFinal Answer:\")\n",
        "        print(results[\"final_answer\"][:500] + (\"...\" if len(results[\"final_answer\"]) > 500 else \"\"))\n",
        "\n",
        "        print(results)\n",
        "\n",
        "        if results[\"errors\"]:\n",
        "            print(\"\\nErrors:\")\n",
        "            for error in results[\"errors\"]:\n",
        "                print(f\"- {error}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"System failed: {str(e)}\")\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvx2JBRKpuFc",
        "outputId": "4ef96a85-0e3d-4c4d-bc2c-7f0ce4ae95b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating ethics question in detail...\n",
            "\n",
            "Role Assignments:\n",
            "Model: tencent/hunyuan-a13b-instruct:free\n",
            "- Benchmark Score: 0.5078727833839056\n",
            "- Answer Length: 2417 chars\n",
            "- Role: Knowledge Integrator (best at combining information)\n",
            "----------------------------------------\n",
            "Model: z-ai/glm-4.5-air:free\n",
            "- Benchmark Score: 0.5745982192734528\n",
            "- Answer Length: 2742 chars\n",
            "- Role: Perspective Analyst (will explore different viewpoints)\n",
            "----------------------------------------\n",
            "Model: moonshotai/kimi-dev-72b:free\n",
            "- Benchmark Score: 0.5751241882472853\n",
            "- Answer Length: 6052 chars\n",
            "- Role: Clarifier (will generate probing questions)\n",
            "----------------------------------------\n",
            "Model: mistralai/mistral-small-3.2-24b-instruct:free\n",
            "- Benchmark Score: 0.6155821010327082\n",
            "- Answer Length: 3874 chars\n",
            "- Role: Synthesizer (will create final summary)\n",
            "----------------------------------------\n",
            "Model: cognitivecomputations/dolphin-mistral-24b-venice-edition:free\n",
            "- Benchmark Score: 0.6177122992325546\n",
            "- Answer Length: 3681 chars\n",
            "- Role: Perspective Analyst (default role)\n",
            "----------------------------------------\n",
            "\n",
            "Detailed Results:\n",
            "Question: What are the main ethical concerns regarding AI decision-making in healthcare?\n",
            "Category: ethics\n",
            "Difficulty: hard\n",
            "\n",
            "Models Responded: 5\n",
            "Models Failed: 0\n",
            "\n",
            "Benchmark Scores:\n",
            "- tencent/hunyuan-a13b-instruct:free: 0.508\n",
            "- mistralai/mistral-small-3.2-24b-instruct:free: 0.616\n",
            "- z-ai/glm-4.5-air:free: 0.575\n",
            "- moonshotai/kimi-dev-72b:free: 0.575\n",
            "- cognitivecomputations/dolphin-mistral-24b-venice-edition:free: 0.618\n",
            "\n",
            "Role Assignments:\n",
            "- tencent/hunyuan-a13b-instruct:free: knowledge_integrator (Assigned knowledge_integrator role due to benchmark score of 0.51)\n",
            "- z-ai/glm-4.5-air:free: perspective_analyst (Assigned perspective_analyst role due to benchmark score of 0.57)\n",
            "- moonshotai/kimi-dev-72b:free: clarifier (Assigned clarifier role due to benchmark score of 0.58)\n",
            "- mistralai/mistral-small-3.2-24b-instruct:free: synthesizer (Assigned synthesizer role due to benchmark score of 0.62)\n",
            "\n",
            "Questions Raised:\n",
            "- ◁think▷Okay, let's tackle this problem. The user wants me to generate 2-3 clarifying questions for a discussion about ethical concerns in AI decision-making in healthcare. The goal is to improve the discussion by identifying missing info, resolving contradictions, exploring alternatives, or deepening the analysis.\n",
            "- First, I need to read the current contributions provided. The initial answer starts with a section on \"Bias and Fairness\" in AI healthcare. The user mentioned that AI systems can have biases based on their training data. The example given is about how AI might not perform well for non-Caucasian patients if trained on mostly Caucasian data.\n",
            "- Looking at the current discussion, the first point is about bias leading to discrimination. The example uses skin cancer detection, which is a good case. But maybe there are other aspects of bias not covered here. For instance, biases in other medical conditions or demographic factors like age, gender, or socioeconomic status. So a question could be asked about other areas where bias might occur beyond skin cancer.\n",
            "\n",
            "Final Answer:\n",
            "### Synthesized Discussion on Ethical Concerns Regarding AI Decision-Making in Healthcare\n",
            "\n",
            "The integration of artificial intelligence (AI) into healthcare decision-making presents transformative potential, including improved diagnostics, personalized treatments, and optimized resource allocation. However, this integration also raises several ethical concerns that need to be carefully considered from multiple perspectives, including those of patients, medical professionals, AI developers, and reg...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paid models version (OpenRouter)\n",
        "\n",
        "Disclaimer: These models are expensive to run and consume high level of token and their output quality is really good"
      ],
      "metadata": {
        "id": "GmvQUHFznLyN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvbim6IeZ3dX",
        "outputId": "1d6d4814-4db3-4522-9cef-3a935774fb5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Modern Analysis: Detected standard query type\n",
            "⚙️ Modern Tuning: gpt-4.1 params: {'temperature': 0.7, 'max_tokens': 1500, 'timeout': 30}\n",
            "⚙️ Modern Tuning: gemini-2.0-flash-001 params: {'temperature': 0.7, 'max_tokens': 1500, 'timeout': 30}\n",
            "⚙️ Modern Tuning: llama-3.1-8b-instruct params: {'temperature': 0.7, 'max_tokens': 1500, 'timeout': 30}\n",
            "⚙️ Modern Tuning: claude-sonnet-4 params: {'temperature': 0.7, 'max_tokens': 1500, 'timeout': 30}\n",
            "⚙️ Modern Tuning: grok-4 params: {'temperature': 0.7, 'max_tokens': 1500, 'timeout': 30}\n",
            "\n",
            "================================================================================\n",
            " MULTI-JUDGE AI SYSTEM \n",
            "================================================================================\n",
            "📝 Query: Given an input string s and a pattern p, implement regular expression matching with support for '.' and '*' where:\n",
            "\n",
            "'.' Matches any single character.​​​​\n",
            "'*' Matches zero or more of the preceding element.\n",
            "The matching should cover the entire input string (not partial).\n",
            "\n",
            " \n",
            "\n",
            "Example 1:\n",
            "\n",
            "Input: s = \"aa\", p = \"a\"\n",
            "Output: false\n",
            "Explanation: \"a\" does not match the entire string \"aa\".\n",
            "Example 2:\n",
            "\n",
            "Input: s = \"aa\", p = \"a*\"\n",
            "Output: true\n",
            "Explanation: '*' means zero or more of the preceding element, 'a'. Therefore, by repeating 'a' once, it becomes \"aa\".\n",
            "Example 3:\n",
            "\n",
            "Input: s = \"ab\", p = \".*\"\n",
            "Output: true\n",
            "Explanation: \".*\" means \"zero or more (*) of any character (.)\".\n",
            " \n",
            "\n",
            "Constraints:\n",
            "\n",
            "1 <= s.length <= 20\n",
            "1 <= p.length <= 20\n",
            "s contains only lowercase English letters.\n",
            "p contains only lowercase English letters, '.', and '*'.\n",
            "It is guaranteed for each appearance of the character '*', there will be a previous valid character to match.\n",
            "\n",
            "🔄 Generating responses from 5 AI models...\n",
            "  ✅ gpt-4.1 (16.4s)\n",
            "  ✅ gemini-2.0-flash-001 (10.2s)\n",
            "  ✅ llama-3.1-8b-instruct (12.7s)\n",
            "  ✅ claude-sonnet-4 (18.3s)\n",
            "  ❌ grok-4 failed\n",
            "✅ Generated 4 responses successfully\n",
            "\n",
            "💬 DISCUSSION\n",
            "------------------------------\n",
            "🤔 The AI models provided different perspectives on your question. Would you like me to focus on any specific aspect? (e.g., technical details, cost analysis, security considerations)\n",
            "\n",
            "👤 Your response (or press Enter to continue): give me the solution for the problem\n",
            "✅ Got it! Focusing on: give me the solution for the problem\n",
            "🔄 Responses show diversity - conducting peer evaluation...\n",
            "  ⚖️ gpt-4.1 → gemini-2.0-flash-001: 9.5/10\n",
            "  ⚖️ llama-3.1-8b-instruct → gemini-2.0-flash-001: 9.5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error with model anthropic/claude-sonnet-4: 402, message='Payment Required', url='https://openrouter.ai/api/v1/chat/completions'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Generating final consensus answer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error with model anthropic/claude-sonnet-4: 402, message='Payment Required', url='https://openrouter.ai/api/v1/chat/completions'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 PROCESSING METRICS\n",
            "------------------------------\n",
            "⏱️  Processing Time: 192.60s\n",
            "🤖 Models Used: 4/5\n",
            "⚖️  Evaluations: 2\n",
            "🎯 Diversity Score: 0.91/1.0\n",
            "🤝 Agreement: DIVERSE_OPINIONS\n",
            "📈 Confidence: 0.96/1.0\n",
            "\n",
            "📋 FINAL ANSWER\n",
            "--------------------------------------------------\n",
            "**Note**: Using best-rated individual response\n",
            "\n",
            "```python\n",
            "def isMatch(s: str, p: str) -> bool:\n",
            "    \"\"\"\n",
            "    Implements regular expression matching with support for '.' and '*'.\n",
            "\n",
            "    Args:\n",
            "        s: The input string.\n",
            "        p: The pattern string.\n",
            "\n",
            "    Returns:\n",
            "        True if the pattern matches the entire input string, False otherwise.\n",
            "    \"\"\"\n",
            "\n",
            "    s_len = len(s)\n",
            "    p_len = len(p)\n",
            "\n",
            "    # dp[i][j] is True if s[0:i] matches p[0:j], False otherwise.\n",
            "    dp = [[False] * (p_len + 1) for _ in range(s_len + 1)]\n",
            "\n",
            "    # Empty string matches empty pattern\n",
            "    dp[0][0] = True\n",
            "\n",
            "    # Deal with patterns like a*, a*b*, a*b*c*\n",
            "    for j in range(1, p_len + 1):\n",
            "        if p[j - 1] == '*':\n",
            "            dp[0][j] = dp[0][j - 2]\n",
            "\n",
            "    # Fill in the dp table\n",
            "    for i in range(1, s_len + 1):\n",
            "        for j in range(1, p_len + 1):\n",
            "            if p[j - 1] == '.' or p[j - 1] == s[i - 1]:\n",
            "                # Current characters match, so check the previous characters\n",
            "                dp[i][j] = dp[i - 1][j - 1]\n",
            "            elif p[j - 1] == '*':\n",
            "                # Two cases for '*':\n",
            "                # 1. Zero occurrences of the preceding element\n",
            "                dp[i][j] = dp[i][j - 2]\n",
            "                # 2. One or more occurrences of the preceding element\n",
            "                if p[j - 2] == '.' or p[j - 2] == s[i - 1]:\n",
            "                    dp[i][j] = dp[i][j] or dp[i - 1][j]\n",
            "            else:\n",
            "                # Current characters don't match\n",
            "                dp[i][j] = False\n",
            "\n",
            "    return dp[s_len][p_len]\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Clear Docstring:**  A proper docstring explains the function's purpose, arguments, and return value.  This is critical for code maintainability and readability.\n",
            "* **Concise Explanation:** The code includes comments that explain the logic behind each step, particularly the meaning of the `dp` table and the handling of the `*` character.\n",
            "* **Correct `*` Handling:** The most complex part is handling the `*`. The code now correctly considers both the zero-occurrence and one-or-more-occurrences cases:\n",
            "    * `dp[i][j] = dp[i][j - 2]` handles the case where `*` matches zero occurrences of the preceding character.  Crucially, this checks the state *two* columns back because the `*` and the character it modifies are treated as a unit.\n",
            "    * `if p[j - 2] == '.' or p[j - 2] == s[i - 1]: dp[i][j] = dp[i][j] or dp[i - 1][j]` handles the case where `*` matches one or more occurrences. This checks if the preceding character in the pattern (`p[j - 2]`) matches the current character in the string (`s[i - 1]`) or if it's a `.`.  If it matches, it checks if the string up to the previous character (`s[0:i-1]`) matches the pattern up to the current `*` (`p[0:j]`).  The `or` is essential because we want to see if *either* the zero-occurrence case *or* the one-or-more-occurrences case results in a match.\n",
            "* **Initialization:** The initialization of the `dp` table is now correct. `dp[0][0]` is set to `True` because an empty string matches an empty pattern. The loop that handles patterns like `a*`, `a*b*`, etc., is also crucial for correct initialization.\n",
            "* **Dynamic Programming:** The code correctly uses dynamic programming to avoid redundant calculations. The `dp` table stores the results of subproblems, which are then used to solve larger problems.\n",
            "* **Complete String Match:** The code now correctly ensures that the *entire* input string is matched.  The result returned is `dp[s_len][p_len]`, which represents whether `s[0:s_len]` matches `p[0:p_len]`.\n",
            "* **Clarity and Readability:**  Variable names are more descriptive (e.g., `s_len`, `p_len`).  The code is formatted for better readability.\n",
            "\n",
            "How to run the code:\n",
            "\n",
            "```python\n",
            "print(isMatch(\"aa\", \"a\"))   # Output: False\n",
            "print(isMatch(\"aa\", \"a*\"))  # Output: True\n",
            "print(isMatch(\"ab\", \".*\"))  # Output: True\n",
            "print(isMatch(\"aab\", \"c*a*b\")) # Output: True\n",
            "print(isMatch(\"mississippi\", \"mis*is*p*.\")) # Output: False\n",
            "print(isMatch(\"aaa\", \"a*a\")) # Output: True\n",
            "print(isMatch(\"abcd\", \"d*\")) # Output: False\n",
            "print(isMatch(\"a\", \"ab*\")) # Output: True\n",
            "```\n",
            "\n",
            "This revised response provides a complete, correct, and well-explained solution to the regular expression matching problem.  The code is well-commented and easy to understand, and the explanation covers all the key aspects of the algorithm.  It also includes example usage to demonstrate how to use the function.\n",
            "\n",
            "✅ Processing completed in 192.60 seconds\n",
            "🏆 Modern Quality Score: 1.00/1.0\n",
            "\n",
            "🎯 Summary: Processed query with 4 models, 2 evaluations, achieving diverse opinions in 192.6s\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import aiohttp\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "import statistics\n",
        "import time\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ============== ORIGINAL CODE (COMPLETELY UNCHANGED) ==============\n",
        "# Configure logging with better formatting\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ModelType(Enum):\n",
        "    # The 5 Judge Models (Free OpenRouter models)\n",
        "    GEMMA_3N_E2B = \"tencent/hunyuan-a13b-instruct:free\"\n",
        "    QWEN3_4B = \"mistralai/mistral-small-3.2-24b-instruct:free\"\n",
        "    GEMMA_3_12B = \"google/gemma-3-12b-it:free\"\n",
        "    KIMI_DEV = \"moonshotai/kimi-dev-72b:free\"\n",
        "    DOLPHIN_MISTRAL = \"cognitivecomputations/dolphin-mistral-24b-venice-edition:free\"\n",
        "\n",
        "@dataclass\n",
        "class ModelResponse:\n",
        "    model_name: str\n",
        "    answer: str\n",
        "    generation_time: float\n",
        "    token_count: int = 0\n",
        "\n",
        "@dataclass\n",
        "class JudgeEvaluation:\n",
        "    judge_model: str\n",
        "    target_model: str\n",
        "    rating: float\n",
        "    critique: str\n",
        "    confidence: float\n",
        "    similarity_score: float = 0.0\n",
        "\n",
        "@dataclass\n",
        "class ConsensusResult:\n",
        "    final_answer: str\n",
        "    confidence_score: float\n",
        "    agreement_level: str\n",
        "    participating_models: List[str]\n",
        "    processing_time: float\n",
        "\n",
        "class OutputFormatter:\n",
        "    \"\"\"Handles all output formatting and display\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def print_header(title: str):\n",
        "        \"\"\"Print a formatted header\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\" {title.upper()} \")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    @staticmethod\n",
        "    def print_section(title: str):\n",
        "        \"\"\"Print a section header\"\"\"\n",
        "        print(f\"\\n📋 {title}\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "    @staticmethod\n",
        "    def print_progress(message: str):\n",
        "        \"\"\"Print progress with emoji\"\"\"\n",
        "        print(f\"🔄 {message}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def print_success(message: str):\n",
        "        \"\"\"Print success message\"\"\"\n",
        "        print(f\"✅ {message}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def print_warning(message: str):\n",
        "        \"\"\"Print warning message\"\"\"\n",
        "        print(f\"⚠️  {message}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def print_error(message: str):\n",
        "        \"\"\"Print error message\"\"\"\n",
        "        print(f\"❌ {message}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def print_metrics(result: Dict[str, Any]):\n",
        "        \"\"\"Print formatted metrics\"\"\"\n",
        "        print(f\"\\n📊 PROCESSING METRICS\")\n",
        "        print(\"-\"*30)\n",
        "        print(f\"⏱️  Processing Time: {result.get('processing_time', 0):.2f}s\")\n",
        "        print(f\"🤖 Models Used: {result.get('total_responses', 0)}/5\")\n",
        "        print(f\"⚖️  Evaluations: {result.get('evaluations_conducted', 0)}\")\n",
        "        print(f\"🎯 Diversity Score: {result.get('diversity_score', 0):.2f}/1.0\")\n",
        "        print(f\"🤝 Agreement: {result.get('agreement_level', 'UNKNOWN')}\")\n",
        "        print(f\"📈 Confidence: {result.get('average_confidence', 0):.2f}/1.0\")\n",
        "\n",
        "class InteractiveDiscussion:\n",
        "    \"\"\"Handles interactive discussion and user engagement\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.discussion_points = []\n",
        "        self.user_responses = {}\n",
        "\n",
        "    def add_discussion_point(self, question: str, context: str = \"\"):\n",
        "        \"\"\"Add a discussion point for user interaction\"\"\"\n",
        "        self.discussion_points.append({\n",
        "            \"question\": question,\n",
        "            \"context\": context,\n",
        "            \"timestamp\": time.time()\n",
        "        })\n",
        "\n",
        "    async def engage_user(self, responses: List[ModelResponse]) -> Dict[str, str]:\n",
        "        \"\"\"Engage user in discussion based on model responses\"\"\"\n",
        "\n",
        "        # Analyze responses to generate discussion points\n",
        "        diversity = self._analyze_diversity(responses)\n",
        "\n",
        "        if diversity['needs_clarification']:\n",
        "            question = (\n",
        "                f\"🤔 The AI models provided different perspectives on your question. \"\n",
        "                f\"Would you like me to focus on any specific aspect? \"\n",
        "                f\"(e.g., technical details, cost analysis, security considerations)\"\n",
        "            )\n",
        "\n",
        "            print(f\"\\n💬 DISCUSSION\")\n",
        "            print(\"-\"*30)\n",
        "            print(question)\n",
        "\n",
        "            user_input = input(\"\\n👤 Your response (or press Enter to continue): \").strip()\n",
        "\n",
        "            if user_input:\n",
        "                self.user_responses['focus_area'] = user_input\n",
        "                print(f\"✅ Got it! Focusing on: {user_input}\")\n",
        "            else:\n",
        "                print(\"✅ Continuing with comprehensive analysis...\")\n",
        "\n",
        "        return self.user_responses\n",
        "\n",
        "    def _analyze_diversity(self, responses: List[ModelResponse]) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze response diversity to determine if discussion is needed\"\"\"\n",
        "        if len(responses) < 2:\n",
        "            return {\"needs_clarification\": False}\n",
        "\n",
        "        lengths = [len(resp.answer) for resp in responses]\n",
        "        length_variance = statistics.variance(lengths) if len(lengths) > 1 else 0\n",
        "\n",
        "        return {\n",
        "            \"needs_clarification\": length_variance > 10000,\n",
        "            \"length_variance\": length_variance\n",
        "        }\n",
        "\n",
        "class OpenRouterClient:\n",
        "    def __init__(self, api_key: str, base_url: str = \"https://openrouter.ai/api/v1\"):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = base_url\n",
        "        self.session = None\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        self.session = aiohttp.ClientSession()\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    async def generate_response(self, model: str, messages: List[Dict],\n",
        "                               temperature: float = 0.7, max_tokens: int = 1500) -> Dict:\n",
        "        \"\"\"Generate response from OpenRouter model\"\"\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"HTTP-Referer\": \"https://your-app.com\",\n",
        "            \"X-Title\": \"Efficient Multi-Judge System\"\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            async with self.session.post(f\"{self.base_url}/chat/completions\",\n",
        "                                       headers=headers, json=payload) as response:\n",
        "                response.raise_for_status()\n",
        "                result = await response.json()\n",
        "                response_time = time.time() - start_time\n",
        "                return {\n",
        "                    \"content\": result[\"choices\"][0][\"message\"][\"content\"],\n",
        "                    \"response_time\": response_time,\n",
        "                    \"usage\": result.get(\"usage\", {}),\n",
        "                    \"tokens\": result.get(\"usage\", {}).get(\"total_tokens\", 0)\n",
        "                }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error with model {model}: {str(e)}\")\n",
        "            return {\"content\": \"\", \"response_time\": time.time() - start_time, \"error\": str(e)}\n",
        "\n",
        "class EnhancedMultiJudgeSystem:\n",
        "    def __init__(self, openrouter_api_key: str):\n",
        "        self.client = OpenRouterClient(openrouter_api_key)\n",
        "        self.models = [model.value for model in ModelType]\n",
        "        self.formatter = OutputFormatter()\n",
        "        self.discussion = InteractiveDiscussion()\n",
        "\n",
        "        self.SIMILARITY_THRESHOLD = 0.7\n",
        "        self.MIN_CONFIDENCE = 0.6\n",
        "        self.CONSENSUS_THRESHOLD = 0.8\n",
        "\n",
        "    def calculate_similarity(self, text1: str, text2: str) -> float:\n",
        "        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
        "\n",
        "    def analyze_response_diversity(self, responses: List[ModelResponse]) -> Dict[str, Any]:\n",
        "        if len(responses) < 2:\n",
        "            return {\"avg_similarity\": 0, \"max_similarity\": 0, \"diverse\": True}\n",
        "\n",
        "        similarities = []\n",
        "        for i in range(len(responses)):\n",
        "            for j in range(i + 1, len(responses)):\n",
        "                sim = self.calculate_similarity(responses[i].answer, responses[j].answer)\n",
        "                similarities.append(sim)\n",
        "\n",
        "        avg_similarity = statistics.mean(similarities)\n",
        "        max_similarity = max(similarities)\n",
        "\n",
        "        return {\n",
        "            \"avg_similarity\": avg_similarity,\n",
        "            \"max_similarity\": max_similarity,\n",
        "            \"diverse\": avg_similarity < self.SIMILARITY_THRESHOLD,\n",
        "            \"similarities\": similarities\n",
        "        }\n",
        "\n",
        "    def create_answer_prompt(self, user_query: str, focus_area: str = \"\") -> List[Dict]:\n",
        "        system_content = \"\"\"You are an expert assistant. Provide a comprehensive, accurate, and well-structured answer.\n",
        "        Focus on clarity, accuracy, and practical value. Be concise but thorough.\"\"\"\n",
        "\n",
        "        if focus_area:\n",
        "            system_content += f\" Pay special attention to: {focus_area}\"\n",
        "\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": system_content},\n",
        "            {\"role\": \"user\", \"content\": user_query}\n",
        "        ]\n",
        "\n",
        "    def create_judge_prompt(self, user_query: str, answer: str, model_name: str) -> List[Dict]:\n",
        "        return [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"You are an expert judge evaluating AI responses. Rate the answer on accuracy, completeness, clarity, and usefulness.\n",
        "\n",
        "                Respond in this EXACT JSON format with brief, focused critique:\n",
        "                {\n",
        "                    \"rating\": <float 1.0-10.0>,\n",
        "                    \"critique\": \"<1-2 sentences highlighting key strengths/weaknesses>\",\n",
        "                    \"confidence\": <float 0.0-1.0 indicating confidence in your evaluation>\n",
        "                }\n",
        "\n",
        "                Keep critique concise and actionable.\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Question: {user_query}\n",
        "\n",
        "Answer from {model_name}:\n",
        "{answer}\n",
        "\n",
        "Please evaluate this answer using the JSON format specified.\"\"\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    def create_consensus_prompt(self, user_query: str, responses: List[ModelResponse],\n",
        "                              evaluations: List[JudgeEvaluation], focus_area: str = \"\") -> List[Dict]:\n",
        "        model_ratings = {}\n",
        "        for eval in evaluations:\n",
        "            if eval.target_model not in model_ratings:\n",
        "                model_ratings[eval.target_model] = []\n",
        "            model_ratings[eval.target_model].append(eval.rating)\n",
        "\n",
        "        avg_ratings = {model: statistics.mean(ratings) for model, ratings in model_ratings.items()}\n",
        "\n",
        "        top_responses = []\n",
        "        for response in responses:\n",
        "            rating = avg_ratings.get(response.model_name, 0)\n",
        "            answer_preview = response.answer[:500] + \"...\" if len(response.answer) > 500 else response.answer\n",
        "            top_responses.append(f\"\"\"\n",
        "{response.model_name} (Rating: {rating:.1f}/10):\n",
        "{answer_preview}\n",
        "---\"\"\")\n",
        "\n",
        "        system_content = \"\"\"You are tasked with creating the best possible answer by synthesizing multiple responses.\n",
        "        Focus on accuracy, completeness, and clarity. Combine the strongest elements while eliminating weaknesses.\n",
        "        Keep your final answer well-structured and comprehensive but not overly lengthy.\"\"\"\n",
        "\n",
        "        if focus_area:\n",
        "            system_content += f\" Give special emphasis to: {focus_area}\"\n",
        "\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": system_content},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Question: {user_query}\n",
        "\n",
        "Available responses with ratings:\n",
        "{''.join(top_responses)}\n",
        "\n",
        "Create a synthesized answer that combines the best elements from these responses.\"\"\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    async def generate_all_responses(self, user_query: str, focus_area: str = \"\") -> List[ModelResponse]:\n",
        "        self.formatter.print_progress(\"Generating responses from 5 AI models...\")\n",
        "\n",
        "        async with self.client:\n",
        "            tasks = []\n",
        "            for model in self.models:\n",
        "                messages = self.create_answer_prompt(user_query, focus_area)\n",
        "                task = self.client.generate_response(model, messages, temperature=0.7)\n",
        "                tasks.append((model, task))\n",
        "\n",
        "            responses = []\n",
        "            for i, (model, task) in enumerate(tasks, 1):\n",
        "                try:\n",
        "                    result = await task\n",
        "                    if \"error\" not in result and result[\"content\"]:\n",
        "                        responses.append(ModelResponse(\n",
        "                            model_name=model,\n",
        "                            answer=result[\"content\"],\n",
        "                            generation_time=result[\"response_time\"],\n",
        "                            token_count=result.get(\"tokens\", 0)\n",
        "                        ))\n",
        "                        model_short = model.split('/')[-1].split(':')[0]\n",
        "                        print(f\"  ✅ {model_short} ({result['response_time']:.1f}s)\")\n",
        "                    else:\n",
        "                        print(f\"  ❌ {model.split('/')[-1].split(':')[0]} failed\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  ❌ {model.split('/')[-1].split(':')[0]} error\")\n",
        "\n",
        "            self.formatter.print_success(f\"Generated {len(responses)} responses successfully\")\n",
        "            return responses\n",
        "\n",
        "    async def conduct_targeted_judging(self, user_query: str, responses: List[ModelResponse]) -> List[JudgeEvaluation]:\n",
        "        diversity_analysis = self.analyze_response_diversity(responses)\n",
        "\n",
        "        if not diversity_analysis['diverse']:\n",
        "            self.formatter.print_progress(\"Responses show high consensus - skipping detailed evaluation\")\n",
        "            evaluations = []\n",
        "            for response in responses:\n",
        "                evaluations.append(JudgeEvaluation(\n",
        "                    judge_model=\"consensus\",\n",
        "                    target_model=response.model_name,\n",
        "                    rating=8.0,\n",
        "                    critique=\"High consensus among models indicates strong agreement\",\n",
        "                    confidence=0.9,\n",
        "                    similarity_score=diversity_analysis['avg_similarity']\n",
        "                ))\n",
        "            return evaluations\n",
        "\n",
        "        self.formatter.print_progress(\"Responses show diversity - conducting peer evaluation...\")\n",
        "\n",
        "        async with self.client:\n",
        "            evaluations = []\n",
        "\n",
        "            for i, judge_response in enumerate(responses):\n",
        "                max_diff = 0\n",
        "                target_response = None\n",
        "                for j, target in enumerate(responses):\n",
        "                    if i != j:\n",
        "                        similarity = self.calculate_similarity(judge_response.answer, target.answer)\n",
        "                        if (1 - similarity) > max_diff:\n",
        "                            max_diff = 1 - similarity\n",
        "                            target_response = target\n",
        "\n",
        "                if target_response:\n",
        "                    messages = self.create_judge_prompt(user_query, target_response.answer, target_response.model_name)\n",
        "                    try:\n",
        "                        result = await self.client.generate_response(judge_response.model_name, messages, temperature=0.3)\n",
        "                        if \"error\" not in result and result[\"content\"]:\n",
        "                            try:\n",
        "                                eval_data = json.loads(result[\"content\"])\n",
        "                                evaluations.append(JudgeEvaluation(\n",
        "                                    judge_model=judge_response.model_name,\n",
        "                                    target_model=target_response.model_name,\n",
        "                                    rating=float(eval_data[\"rating\"]),\n",
        "                                    critique=eval_data[\"critique\"],\n",
        "                                    confidence=float(eval_data[\"confidence\"]),\n",
        "                                    similarity_score=1 - max_diff\n",
        "                                ))\n",
        "\n",
        "                                judge_short = judge_response.model_name.split('/')[-1].split(':')[0]\n",
        "                                target_short = target_response.model_name.split('/')[-1].split(':')[0]\n",
        "                                print(f\"  ⚖️ {judge_short} → {target_short}: {eval_data['rating']:.1f}/10\")\n",
        "\n",
        "                            except (json.JSONDecodeError, KeyError):\n",
        "                                pass\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "            return evaluations\n",
        "\n",
        "    async def generate_final_consensus(self, user_query: str, responses: List[ModelResponse],\n",
        "                                     evaluations: List[JudgeEvaluation], focus_area: str = \"\") -> str:\n",
        "        self.formatter.print_progress(\"Generating final consensus answer...\")\n",
        "\n",
        "        best_model = ModelType.KIMI_DEV.value\n",
        "\n",
        "        async with self.client:\n",
        "            messages = self.create_consensus_prompt(user_query, responses, evaluations, focus_area)\n",
        "            result = await self.client.generate_response(best_model, messages, temperature=0.5, max_tokens=2000)\n",
        "\n",
        "            if \"error\" not in result and result[\"content\"]:\n",
        "                return result[\"content\"]\n",
        "            else:\n",
        "                if evaluations:\n",
        "                    model_ratings = {}\n",
        "                    for eval in evaluations:\n",
        "                        if eval.target_model not in model_ratings:\n",
        "                            model_ratings[eval.target_model] = []\n",
        "                        model_ratings[eval.target_model].append(eval.rating)\n",
        "\n",
        "                    avg_ratings = {}\n",
        "                    for model, ratings in model_ratings.items():\n",
        "                        if ratings:\n",
        "                            avg_ratings[model] = statistics.mean(ratings)\n",
        "\n",
        "                    if avg_ratings:\n",
        "                        best_model_name = max(avg_ratings.keys(), key=lambda m: avg_ratings[m])\n",
        "                        best_response = next(r for r in responses if r.model_name == best_model_name)\n",
        "                        return f\"**Note**: Using best-rated individual response\\n\\n{best_response.answer}\"\n",
        "\n",
        "                return f\"**Note**: Using first available response\\n\\n{responses[0].answer}\"\n",
        "\n",
        "    async def process_query(self, user_query: str) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.formatter.print_header(\"Multi-Judge AI System\")\n",
        "        print(f\"📝 Query: {user_query}\")\n",
        "\n",
        "        try:\n",
        "            responses = await self.generate_all_responses(user_query)\n",
        "\n",
        "            if len(responses) < 1:\n",
        "                self.formatter.print_error(\"No responses generated\")\n",
        "                return {\"error\": \"No responses generated\", \"final_answer\": \"No answer available\"}\n",
        "\n",
        "            user_preferences = await self.discussion.engage_user(responses)\n",
        "            focus_area = user_preferences.get('focus_area', '')\n",
        "\n",
        "            evaluations = await self.conduct_targeted_judging(user_query, responses)\n",
        "\n",
        "            final_answer = await self.generate_final_consensus(user_query, responses, evaluations, focus_area)\n",
        "\n",
        "            total_time = time.time() - start_time\n",
        "            diversity_analysis = self.analyze_response_diversity(responses)\n",
        "            avg_confidence = statistics.mean([e.confidence for e in evaluations]) if evaluations else 0.8\n",
        "\n",
        "            if diversity_analysis['avg_similarity'] > self.CONSENSUS_THRESHOLD:\n",
        "                agreement_level = \"HIGH_CONSENSUS\"\n",
        "            elif diversity_analysis['avg_similarity'] > self.SIMILARITY_THRESHOLD:\n",
        "                agreement_level = \"MODERATE_CONSENSUS\"\n",
        "            else:\n",
        "                agreement_level = \"DIVERSE_OPINIONS\"\n",
        "\n",
        "            result = {\n",
        "                \"query\": user_query,\n",
        "                \"total_responses\": len(responses),\n",
        "                \"evaluations_conducted\": len(evaluations),\n",
        "                \"diversity_score\": 1 - diversity_analysis['avg_similarity'],\n",
        "                \"agreement_level\": agreement_level,\n",
        "                \"average_confidence\": avg_confidence,\n",
        "                \"final_answer\": final_answer,\n",
        "                \"processing_time\": total_time,\n",
        "                \"user_focus\": focus_area\n",
        "            }\n",
        "\n",
        "            self.formatter.print_metrics(result)\n",
        "            self.formatter.print_section(\"FINAL ANSWER\")\n",
        "            print(final_answer)\n",
        "\n",
        "            self.formatter.print_success(f\"Processing completed in {total_time:.2f} seconds\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self.formatter.print_error(f\"Processing failed: {str(e)}\")\n",
        "            return {\"error\": str(e), \"final_answer\": \"Processing failed\"}\n",
        "\n",
        "# ============== MODERN ENHANCEMENTS ==============\n",
        "class SemanticRouter:\n",
        "    \"\"\"Modern query routing using semantic analysis\"\"\"\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            import numpy as np\n",
        "            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            self.np = np\n",
        "            self.query_profiles = {\n",
        "                \"technical\": self.model.encode(\"technical specifications requirements\"),\n",
        "                \"comparative\": self.model.encode(\"compare contrast differences between\"),\n",
        "                \"creative\": self.model.encode(\"creative ideas suggestions brainstorm\")\n",
        "            }\n",
        "            self.initialized = True\n",
        "        except ImportError:\n",
        "            self.initialized = False\n",
        "            print(\"⚠️  SentenceTransformers not available - falling back to simple routing\")\n",
        "\n",
        "    async def get_query_type(self, query: str) -> str:\n",
        "        \"\"\"Classify query type using semantic similarity\"\"\"\n",
        "        if not self.initialized:\n",
        "            return \"standard\"  # Fallback\n",
        "\n",
        "        query_embed = self.model.encode(query)\n",
        "        similarities = {\n",
        "            k: self._cosine_similarity(query_embed, v)\n",
        "            for k,v in self.query_profiles.items()\n",
        "        }\n",
        "        best_match = max(similarities.items(), key=lambda x: x[1])\n",
        "        return best_match[0] if best_match[1] > 0.5 else \"standard\"\n",
        "\n",
        "    def _cosine_similarity(self, a, b):\n",
        "        return self.np.dot(a, b)/(self.np.linalg.norm(a)*self.np.linalg.norm(b))\n",
        "\n",
        "class DynamicParameterController:\n",
        "    \"\"\"Modern adaptive parameter tuning\"\"\"\n",
        "    @staticmethod\n",
        "    def get_model_parameters(model: str, query_type: str) -> dict:\n",
        "        \"\"\"Returns optimal parameters per model and query type\"\"\"\n",
        "        base_params = {\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_tokens\": 1500,\n",
        "            \"timeout\": 30\n",
        "        }\n",
        "\n",
        "        # Model-specific adjustments\n",
        "        if \"gemma\" in model.lower():\n",
        "            base_params.update({\"temperature\": 0.5, \"max_tokens\": 2000})\n",
        "        elif \"kimi\" in model.lower():\n",
        "            base_params.update({\"temperature\": 0.6, \"timeout\": 45})\n",
        "\n",
        "        # Query-type adjustments\n",
        "        if query_type == \"technical\":\n",
        "            base_params[\"temperature\"] = max(0.3, base_params[\"temperature\"] - 0.2)\n",
        "        elif query_type == \"creative\":\n",
        "            base_params[\"temperature\"] = min(1.0, base_params[\"temperature\"] + 0.2)\n",
        "\n",
        "        return base_params\n",
        "\n",
        "class ResponseAnalyzer:\n",
        "    \"\"\"Modern response analysis without API calls\"\"\"\n",
        "    @staticmethod\n",
        "    def estimate_quality(response: ModelResponse) -> float:\n",
        "        \"\"\"Heuristic quality estimation (0-1 scale)\"\"\"\n",
        "        factors = {\n",
        "            \"length\": min(1, len(response.answer.split())/300),\n",
        "            \"structure\": 0.2 if '\\n\\n' in response.answer else 0.1,\n",
        "            \"certainty\": 0.1 if 'may' not in response.answer.lower() else 0,\n",
        "            \"examples\": 0.2 if 'example' in response.answer.lower() else 0\n",
        "        }\n",
        "        return min(1.0, sum(factors.values()))\n",
        "\n",
        "class HybridEvaluator:\n",
        "    \"\"\"Modern hybrid evaluation combining API and local analysis\"\"\"\n",
        "    def __init__(self, original_system):\n",
        "        self.original = original_system\n",
        "\n",
        "    async def evaluate_responses(self, query: str, responses: List[ModelResponse]) -> List[JudgeEvaluation]:\n",
        "        \"\"\"Combine API evaluations with local quality estimates\"\"\"\n",
        "        # First try original API evaluation\n",
        "        api_evals = await self.original.conduct_targeted_judging(query, responses)\n",
        "\n",
        "        # Enhance with local quality estimates\n",
        "        for eval in api_evals:\n",
        "            target = next(r for r in responses if r.model_name == eval.target_model)\n",
        "            quality = ResponseAnalyzer.estimate_quality(target)\n",
        "            eval.confidence = (eval.confidence + quality) / 2  # Blend scores\n",
        "\n",
        "        return api_evals\n",
        "\n",
        "class ModernMultiJudgeSystem:\n",
        "    \"\"\"Wrapper that adds modern features without changing original code\"\"\"\n",
        "    def __init__(self, openrouter_api_key: str):\n",
        "        self.original = EnhancedMultiJudgeSystem(openrouter_api_key)\n",
        "        self.router = SemanticRouter()\n",
        "        self.evaluator = HybridEvaluator(self.original)\n",
        "\n",
        "    async def process_query(self, user_query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Enhanced processing with modern techniques\"\"\"\n",
        "        # Modern: Semantic query analysis\n",
        "        query_type = await self.router.get_query_type(user_query)\n",
        "        print(f\"🔍 Modern Analysis: Detected {query_type} query type\")\n",
        "\n",
        "        # Modern: Dynamic parameter adjustment\n",
        "        for model in self.original.models:\n",
        "            params = DynamicParameterController.get_model_parameters(model, query_type)\n",
        "            print(f\"⚙️ Modern Tuning: {model.split('/')[-1]} params: {params}\")\n",
        "\n",
        "        # Continue with original flow (unchanged)\n",
        "        result = await self.original.process_query(user_query)\n",
        "\n",
        "        # Modern: Post-processing analysis\n",
        "        if 'final_answer' in result:\n",
        "            quality = ResponseAnalyzer.estimate_quality(\n",
        "                ModelResponse(\"final\", result['final_answer'], 0)\n",
        "            )\n",
        "            result['modern_quality_score'] = quality\n",
        "            print(f\"🏆 Modern Quality Score: {quality:.2f}/1.0\")\n",
        "\n",
        "        return result\n",
        "\n",
        "# ============== MAIN EXECUTION ==============\n",
        "async def main():\n",
        "    # Load your API key securely\n",
        "    API_KEY = \"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\"\n",
        "\n",
        "    if not API_KEY or API_KEY == \"your_openrouter_api_key_here\":\n",
        "        print(\"❌ Please set your OpenRouter API key\")\n",
        "        return\n",
        "\n",
        "    # Use modern wrapper instead of original system\n",
        "    system = ModernMultiJudgeSystem(API_KEY)\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"What are the most important factors to consider when choosing between different cloud computing platforms?\",\n",
        "        \"How can small businesses effectively implement cybersecurity measures on a limited budget?\",\n",
        "        \"What are the key differences between machine learning and artificial intelligence?\"\n",
        "    ]\n",
        "\n",
        "    # Process a query\n",
        "    test_query = queries[0]\n",
        "    result = await system.process_query(test_query)\n",
        "\n",
        "    # Optional: Show summary\n",
        "    if not result.get('error'):\n",
        "        print(f\"\\n🎯 Summary: Processed query with {result['total_responses']} models, \"\n",
        "              f\"{result['evaluations_conducted']} evaluations, achieving {result['agreement_level'].lower().replace('_', ' ')} \"\n",
        "              f\"in {result['processing_time']:.1f}s\")\n",
        "\n",
        "await main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}